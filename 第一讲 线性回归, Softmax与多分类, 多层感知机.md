## 第一讲 线性回归, Softmax与多分类, 多层感知机

### 线性回归

做模型的时候，要决定模型的形式，损失函数，优化函数，评价指标等等

如果使用全部的数据集进行梯度下降，会导致计算非常缓慢。如果使用随机梯度下降，因为每次只使用一个样本点，导致有跳出局部最优点的可能性，收敛比较慢。所以当数据量比较大的时候，使用mini-batch方法进行梯度下降。

### Softmax与多分类

在训练神经网络的时候，在前几个epoch可能会出现训练集误差大于测试集误差的情况。原因是，在每个epoch里面，模型的参数会被更新batch_num次，训练集的误差由多次预测结果混合得出。但是测试集误差只使用了epoch结束时最近一次模型参数来计算。在训练初期，模型还未很好地学习样本，所以epoch结束时候的模型效果会比结束前更好，因此测试集会表现比训练集更好一些。

### 多层感知机

当使用神经网络的时候，需要考虑网络架构，具体包含网络深度，每层网络的神经元数量，激活函数，网络的结构。

因为线性变换具有线性性质，所以需要使用非线性的激活函数。通常使用ReLU函数，ReLU的计算量比sigmoid更小。但是，ReLU只能使用在隐藏层中。用于分类器时，通常sigmoid和tanh效果比较好。当神经网络层数比较多，由于sigmoid和tanh会遇到梯度消失的问题，通常使用ReLU。

具体使用pytorch手写神经网络的时候，需要梯度清零，因为api在反向传播的时候自动累计梯度。

对于Fashion-MNIST数据集，每个图片是28*28的像素。如果使用一层隐藏层，神经元个数256的模型。调节学习率可以取得85%的准确率。如果使用两个隐藏层，神经元个数为256。调节学习率可以取得87%的准确率。kaggle上有一些kernel使用CNN轻松地达到了93%的准确率。













