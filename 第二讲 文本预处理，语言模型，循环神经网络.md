## 第二讲 文本预处理，语言模型，循环神经网络

### 文本预处理

一般来说，如果使用之前的文本预测之后的文本，这类问题属于序列预测问题。文本本身不能用来直接建模，需要进行分词处理，然后才能当做模型的输入。

Token指的是表征，如果token是单词，那么按照词的级别进行分词，如果token是句子，那么按照句子的级别进行分词。具体根据需要选取。

如果自己分词，可以根据语料库来建立索引和单词的映射，然后把建模的文本映射为一个one_hot向量，输入给模型。

目前比较好的分词工具是spacy和NLTK，中文用jieba。

### 语言模型

本节介绍语言模型，主要是元语法(-gram)。一段自然语言文本可以看做是一个离散的时间序列，给定一个长度为$T$的次序列，语言模型的目标就是评估该序列是否合理，即计算该序列出现的概率。

#### n元语法

序列长度增加，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。$n$元语法通过马尔可夫假设简化模型，马尔科夫假设是指一个词的出现只与前面$n$个词相关，即$n$阶马尔可夫链（Markov chain of order $n$），如果$n=1$，那么有$P(w_3 \mid w_1, w_2) = P(w_3 \mid w_2)$。基于$n-1$阶马尔可夫链，我们可以将语言模型改写为


$$
P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^T P(w_t \mid w_{t-(n-1)}, \ldots, w_{t-1}) .
$$

以上也叫$n$元语法（$n$-grams），它是基于$n - 1$阶马尔可夫链的概率语言模型。



元语法的缺陷是：

1. 参数空间太大。随着n指数增长
2. 数据稀疏。大部分为虚词和符号

对于时序数据的采样分为：

1. 随机采样
   根据batch_size和时间步长t，可以知道有多少有效的样本和标签。然后随机采样到每个batch中。
2. 相邻采样
   根据batch_size和时间步长t，可以知道有多少有效的样本和标签。把相邻的样本分配到不同的batch中。

在随机采样中，每个样本是原始序列上任意截取的一段序列。相邻的两个随机小批量在原始序列中不一定相邻。所以，无法用一个小批量的最终步的隐藏状态来初始化下一个小批量的隐藏状态。在训练模型的时候，每次随机采样前都需要重新初始化隐藏状态。

在相邻采样中，我们可以用一个小批量的最终时间步的隐藏状态来初始化下一个小批量的隐藏状态，从而使得下一个小批量的输出也取决于当前小批量的输入，并且如此循环下去，这是优点。另一方面，当多个相邻小批量通过传递隐藏状态连接起来的时候，模型参数的梯度计算将依赖所有串联起来的小批量序列。在同一迭代周期中，随着迭代次数的增加，梯度的计算开小会越来越大。

### 循环神经网络

待续。。。











